{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOboON4591Xx5SoRPxdjlNM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anurag-sid/Natural-Language-Processing-NLP-/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python code use the 're' module to search for a specific pattern or substring in a given text/string."
      ],
      "metadata": {
        "id": "25m0nGhedUjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LsgaIeyJida",
        "outputId": "5db7df65-a8f3-4214-f4ac-b79e6d905979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found a match: am\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"i am a data scientist\"\n",
        "pattern = \"am\"\n",
        "\n",
        "match = re.search(pattern, text)\n",
        "\n",
        "if match:\n",
        "    print(\"Found a match:\", match.group())\n",
        "else:\n",
        "    print(\"Match not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code replaces the word \"analyst\" with \"scientist\" in the string \"i am a data analyst\". Using the re.sub() function from the re module, it creates a new string where the word \"analyst\" is substituted. The original and the modified text are then printed."
      ],
      "metadata": {
        "id": "k719WaHRdpQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" i am a data analyst\"\n",
        "pattern = \"analyst\"\n",
        "replacement = \"scientist\"\n",
        "new_text = re.sub(pattern, replacement, text)\n",
        "print(\"Original Text :\",text)\n",
        "print(\"new Text :\",new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s07Ot-VrJ666",
        "outputId": "278ad6de-484d-479b-f975-440f658b5837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text :  i am a data analyst\n",
            "new Text :  i am a data scientist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code removes all punctuation marks from the string \"Hey, This is best :for !us;\" using a regular expression. The pattern r'[^\\w\\s]' matches any character that is not a word character (letters, digits, and underscores) or whitespace. The re.sub() function replaces these non-word, non-whitespace characters with an empty string."
      ],
      "metadata": {
        "id": "bKYzmDuSd5rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = \"Hey, This is best :for !us;\"\n",
        "print(\"The original string is :\"+test1)\n",
        "res = re.sub(r'[^\\w\\s]','',test1)\n",
        "print(\"The String after punctuation filter :\"+res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRxbBXJnKpHt",
        "outputId": "2c7b7cc8-d057-439a-d7ef-13d78d9077eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original string is :Hey, This is best :for !us;\n",
            "The String after punctuation filter :Hey This is best for us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK : Natural Language Toolkit"
      ],
      "metadata": {
        "id": "3DWzVv5MM83Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maHKYmtkORR_",
        "outputId": "fbd10213-e79e-4ee6-fbd6-5fef980b3b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will tokenize the input text \"This is an example sentence. Another sentence is here.\" into individual words and punctuation marks using nltk.word_tokenize()."
      ],
      "metadata": {
        "id": "f-KbDM_vd9-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "text = \"This is an example sentence. Another sentence is here.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bufNNGm0L1y_",
        "outputId": "359e5e86-48f1-4e2c-aca5-114c9e98c6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', '.', 'Another', 'sentence', 'is', 'here', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the nltk.sent_tokenize() function to split the given text into sentences. The input text \"This is the first sentence.This is the second sentence.And this is the third senetence.\" will be divided into separate sentences based on punctuation marks (like periods)."
      ],
      "metadata": {
        "id": "c5c2U-VseWsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is the first sentence.This is the second sentence.And this is the third senetence.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSekQsyNoJk",
        "outputId": "2d649b74-7886-4346-9eeb-d4793b62e091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.This is the second sentence.And this is the third senetence.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code tokenizes the input text \"This is an example sentence.\" into words and then generates bigrams (pairs of consecutive words) and trigrams (triplets of consecutive words) using the nltk.util.ngrams function."
      ],
      "metadata": {
        "id": "t_I-QRGbemZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "text = \"This is an example sentence.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"Original Text :\",text)\n",
        "print(\"Tokens :\",tokens)\n",
        "bigrams = list(ngrams(tokens,2))\n",
        "print(\"Bigrams :\",bigrams)\n",
        "trigrams = list(ngrams(tokens,3))\n",
        "print(\"Trigrams :\",trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eET2ndNJPMLV",
        "outputId": "8a7e0206-3e69-4926-c875-0a108c856677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text : This is an example sentence.\n",
            "Tokens : ['This', 'is', 'an', 'example', 'sentence', '.']\n",
            "Bigrams : [('This', 'is'), ('is', 'an'), ('an', 'example'), ('example', 'sentence'), ('sentence', '.')]\n",
            "Trigrams : [('This', 'is', 'an'), ('is', 'an', 'example'), ('an', 'example', 'sentence'), ('example', 'sentence', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the Porter Stemmer from the NLTK library to reduce the word \"running\" to its stemmed form."
      ],
      "metadata": {
        "id": "AxQ9eDlXes46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "word = \"running\"\n",
        "stemmed_words = stemmer.stem(word)\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ1Nb5efPbv9",
        "outputId": "18458760-e473-40df-d698-fe4a9af29928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the WordNet Lemmatizer from the NLTK library to lemmatize the word \"running\" with the part of speech (POS) specified as a verb (\"v\"). Lemmatization involves reducing a word to its base or dictionary form, unlike stemming, which simply truncates prefixes or suffixes."
      ],
      "metadata": {
        "id": "6gDF8hMHe2E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word = \"running\"\n",
        "lemmatized_word = lemmatizer.lemmatize(word,pos=\"v\")\n",
        "print(lemmatized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyAEmZJMQw67",
        "outputId": "b7acc28e-301d-428e-b44d-93ff72a2ee26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code removes stop words (common, less meaningful words like \"is\", \"an\", \"with\", etc.) from the input text using the NLTK library's stopwords corpus. It tokenizes the text, filters out stop words, and then joins the remaining words back into a sentence."
      ],
      "metadata": {
        "id": "eZwA2eWxe66D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "filtered_words = \" \".join(filtered_words)\n",
        "print(\"Original Text :\",text)\n",
        "print(\"Filtered Text :\",filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wiqV_dcR4s4",
        "outputId": "8bb9bcba-cbf4-4ff9-fc1b-bb5b9bfec252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text : This is an example sentence with some stop words.\n",
            "Filtered Text : example sentence stop words .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'averaged_perceptron_tagger' resource for English\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"The cat is sitting on the mat.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the results\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAD7n0eVTgDL",
        "outputId": "5e72286a-d6de-4a05-f286-8f44062ea9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "    Tokenization: The text is tokenized into words using word_tokenize().\n",
        "\n",
        "    POS Tagging: The pos_tag() function assigns part-of-speech tags to each token.\n",
        "\n",
        "    Named Entity Recognition (NER): The ne_chunk() function identifies named entities (like organizations, locations, and persons) in the tagged tokens.\n",
        "\n",
        "    Result: The named entities are printed with their labels (e.g., \"PERSON\" for a person's name, \"GPE\" for a geopolitical entity like a location, \"ORGANIZATION\" for a company)."
      ],
      "metadata": {
        "id": "Ztdh7KFxfKzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, ne_chunk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "ne = ne_chunk(pos_tags)\n",
        "for chunk in ne:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp3FPfnIVavU",
        "outputId": "4b1872a5-7aa9-46d8-e239-45d9f01baad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERSON Apple\n",
            "ORGANIZATION Inc.\n",
            "PERSON Steve Jobs\n",
            "GPE Cupertino\n",
            "GPE California\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the SentimentIntensityAnalyzer from the NLTK library to analyze the sentiment of a given text. The polarity_scores() function calculates a set of sentiment scores, including positive, negative, neutral, and compound scores."
      ],
      "metadata": {
        "id": "2azqr8q4fX8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "text = \"I hate this product! It would not work properly and I would definitely reccomend it to anyone\"\n",
        "scores = sid.polarity_scores(text)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMItuC41XpNw",
        "outputId": "b1bf56b1-e79a-42a5-e464-42fb8ba6e389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.203, 'neu': 0.66, 'pos': 0.137, 'compound': -0.3164}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}